# # -*- coding: utf-8 -*-
# """Delhi's AQI 2025.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1tWZYwKdsy5vS_LiL-OzrZsTKswPy1HmB
# """

# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns

# from statsmodels.tsa.arima.model import ARIMA

# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# from sklearn.metrics import accuracy_score, classification_report
# from sklearn.metrics import mean_squared_error, r2_score

# from sklearn.linear_model import LogisticRegression, LinearRegression
# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
# from sklearn.svm import SVC, SVR

# import warnings
# warnings.filterwarnings('ignore')

# df=pd.read_csv("delhi-weather-aqi-2025.csv")
# df.head()

# # Line 32-33: Merges the separate "Date" and "Time" columns into one single timeline so Python can understand the chronological order of the air pollution.
# df['datetime']=pd.to_datetime(df['date_ist'] + ' ' + df['time_ist'], dayfirst=True)
# df=df.sort_values('datetime')

# df.head()

# ts=df.groupby('datetime')['aqi_index'].mean()
# ts.head()

# plt.figure(figsize=(12,4))
# plt.plot(ts)
# plt.title("Hourly Average AQI - Delhi 2025")
# plt.xlabel("Time")
# plt.ylabel("AQI")
# plt.show()

# train_size=int(len(ts) * 0.8)
# train, test=ts[:train_size], ts[train_size:]

# model=ARIMA(train, order=(5,1,2))
# model_fit=model.fit()
# print(model_fit.summary())

# forecast=model_fit.forecast(steps=len(test))

# plt.figure(figsize=(12,4))
# plt.plot(test.index, test, label='Actual')
# plt.plot(test.index, forecast, label='Forecast')
# plt.legend()
# plt.title("ARIMA Forecast vs Actual AQI")
# plt.show()

# future_24=model_fit.forecast(steps=24)
# print("Next 24 Hours AQI Prediction:")
# print(future_24)

# # features=['temp_c', 'humidity', 'pm2_5', 'pm10', 'no2', 'co'] previoys features 
# features = ['temp_c', 'humidity', 'no2', 'co']
# X=df[features]

# def label_aqi(x):
#     if x <= 50:
#         return 0
#     elif x <= 100:
#         return 1
#     elif x <= 200:
#         return 2
#     elif x <= 300:
#         return 3
#     elif x <= 400:
#         return 4
#     elif x <= 500:
#         return 5
#     else:
#         return 6

# df['aqi_cat']=df['aqi_index'].apply(label_aqi)
# y_cls=df['aqi_cat']

# X_train, X_test, y_train, y_test=train_test_split(
#     X, y_cls, test_size=0.2, random_state=42
# )

# X_test.shape
# X_test.head()

# y_test.shape
# y_test.head()

# lr=LogisticRegression(max_iter=1000)
# lr.fit(X_train, y_train)
# pred_lr=lr.predict(X_test)

# print("Logistic Accuracy:", accuracy_score(y_test, pred_lr))
# print(classification_report(y_test, pred_lr))

# X_test


# # Line 98-99: The "Random Forest" builds 100 "decision trees" to vote on which AQI category is most likely based on your sensor data.
# model_rf=RandomForestClassifier(n_estimators=100)
# model_rf.fit(X_train, y_train)
# pred_model_rf=model_rf.predict(X_test)

# print("model_rf Accuracy:", accuracy_score(y_test, pred_model_rf))
# print(classification_report(y_test, pred_model_rf))

# pred_model_rf = model_rf.predict([[19.0, 89, 100.8, 140.6, 34.2, 1512]])
# print(pred_model_rf)

# # scaler=StandardScaler()
# # X_scaled=scaler.fit_transform(X)

# # Better scaling practice
# scaler = StandardScaler()
# X_train_s = scaler.fit_transform(X_train) # Fit ONLY on training data
# X_test_s = scaler.transform(X_test)       # Use the same 'mean' for testing

# X_train_s, X_test_s, y_train_s, y_test_s=train_test_split(
#     X_scaled, y_cls, test_size=0.2, random_state=42
# )

# svm=SVC()
# svm.fit(X_train_s, y_train_s)
# pred_svm=svm.predict(X_test_s)

# print("SVM Accuracy:", accuracy_score(y_test_s, pred_svm))

# y_reg=df['aqi_index']

# X_train_r, X_test_r, y_train_r, y_test_r=train_test_split(
#     X, y_reg, test_size=0.2, random_state=42
# )

# lr_reg=LinearRegression()
# lr_reg.fit(X_train_r, y_train_r)
# pred_lr_r=lr_reg.predict(X_test_r)

# print("Linear RMSE:", np.sqrt(mean_squared_error(y_test_r, pred_lr_r)))
# print("Linear R2:", r2_score(y_test_r, pred_lr_r))

# model_rf_reg=RandomForestRegressor(n_estimators=100)
# model_rf_reg.fit(X_train_r, y_train_r)
# pred_model_rf_r=model_rf_reg.predict(X_test_r)

# print("model_rf RMSE:", np.sqrt(mean_squared_error(y_test_r, pred_model_rf_r)))
# print("model_rf R2:", r2_score(y_test_r, pred_model_rf_r))

# svr=SVR()
# svr.fit(X_train_s, y_train_r[:len(X_train_s)])
# pred_svr=svr.predict(X_test_s)

# print("SVR RMSE:", np.sqrt(mean_squared_error(y_test_s, pred_svr)))

# import pickle
# import os

# # Create the folder if it doesn't exist
# if not os.path.exists('app/models'):
#     os.makedirs('app/models')


# # Save the Random Forest Classifier
# with open('app/models/model_rf.pkl', 'wb') as f:
#     pickle.dump(model_rf, f)

# # Save the Scaler (since you used X_scaled for some models, 
# # ensure you save the one used for RF if applicable)
# with open('app/models/scaler.pkl', 'wb') as f:
#     pickle.dump(scaler, f)

# print("Random Forest model saved successfully!")

# """Conclusion
# This project successfully applied both time-series and machine learning techniques to analyze air quality in Delhi.
# ARIMA forecasting captured short-term AQI trends and enabled accurate next-day predictions.
# Manchine learning models futher improved predictive pemodel_rformance by incorporating weather and pollutant variables, with Random Forest achieving the best results in both classification and regression.
# The analysis confirms that particulate matter, particularly PM2.5 and PM10, is the dominant factor influencing AQI levels.
# The combined modeling approach can support real-time pollution monitoring, early warning systems, and informed urbam environmental planning.

# **Reasoning**:
# The previous code block failed because the `MinMaxScaler` object was named `scaler` (not `scaler_lstm`), which was then used incorrectly. I need to replace `scaler_lstm` with `scaler` in the current code block to correctly inverse transform the data.
# """

# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt

# # Make predictions on the test set
# # test_predict = model_lstm.predict(X_test_lstm)

# # Inverse transform the predictions to the original scale
# test_predict = scaler.inverse_transform(test_predict)
# y_test_lstm_inversed = scaler.inverse_transform(y_test_lstm.reshape(-1, 1))

# # Calculate RMSE for the test predictions
# test_rmse = np.sqrt(mean_squared_error(y_test_lstm_inversed, test_predict))
# print(f"Test RMSE: {test_rmse}")

# # Forecast future values (next 34 hours as specified in the main task)
# # Get the last 'look_back' values from the scaled time series to start forecasting
# last_input = ts_scaled[-look_back:]
# last_input = last_input.reshape(1, look_back, 1)

# forecast_steps = 34
# future_forecast = []

# for _ in range(forecast_steps):
#     # Predict the next step
#     next_prediction_scaled = model_lstm.predict(last_input)
#     future_forecast.append(next_prediction_scaled[0, 0])

#     # Update the input sequence for the next prediction
#     # Remove the first element and add the new prediction to the end
#     new_input = np.append(last_input[:, 1:, :], next_prediction_scaled.reshape(1, 1, 1), axis=1)
#     last_input = new_input

# # Inverse transform the future forecast to the original scale
# future_forecast_inversed = scaler.inverse_transform(np.array(future_forecast).reshape(-1, 1))

# print("Next 34 hours AQI forecast:")
# # Create a datetime index for the forecasted values
# last_timestamp = ts.index[-1]
# forecast_index = pd.date_range(start=last_timestamp + pd.Timedelta(hours=1), periods=forecast_steps, freq='H')
# forecast_series = pd.Series(future_forecast_inversed.flatten(), index=forecast_index)
# print(forecast_series)

# # Plot the actual values, test predictions, and future forecast
# plt.figure(figsize=(15, 6))
# plt.plot(ts.index, ts.values, label='Actual AQI')

# # For plotting test predictions, align them with the actual test data's index
# # The test_predict corresponds to X_test_lstm, which starts after the training set
# # The index for test predictions should start after the last training data point in ts

# # Determine the index for the test set in the original `ts` series
# start_test_index_in_ts = len(ts) - len(y_test_lstm)

# # Create an index for the test_predict by slicing the original ts index
# test_prediction_index = ts.index[start_test_index_in_ts:start_test_index_in_ts + len(test_predict)]

# plt.plot(test_prediction_index, test_predict, label='Test Prediction', linestyle='--')
# plt.plot(forecast_index, future_forecast_inversed, label='Future Forecast', linestyle=':', color='green')

# plt.title('AQI Time Series Forecast with LSTM')
# plt.xlabel('Date and Time')
# plt.ylabel('AQI Index')
# plt.legend()
# plt.grid(True)
# plt.show()

# """## Summary:

# ### Data Analysis Key Findings

# *   **Data Preparation:** The time series data was successfully scaled using `MinMaxScaler` and transformed into a sequential dataset with a look-back period of 24 hours. The data was split into training and testing sets, with 6988 samples for training and 1748 samples for testing.
# *   **Model Pemodel_rformance:** An LSTM model was built and trained. The Root Mean Squared Error (RMSE) for predictions on the test set was approximately 4.166.
# *   **Future Forecast:** The model generated a 34-hour future forecast for AQI, which visually indicated a decreasing trend.
# *   **Visualization:** The actual AQI values, test predictions, and the 34-hour future forecast were successfully plotted, allowing for a clear visual assessment of the model's pemodel_rformance and future outlook.

# ### Insights or Next Steps

# *   The current LSTM model provides a good baseline for AQI forecasting, achieving a test RMSE of approximately 4.166.
# *   To potentially enhance model accuracy, consider experimenting with different LSTM architectures (e.g., adding more layers or units), optimizing hyperparameters, or incorporating additional relevant features like weather conditions.

# """

# # 1. Import necessary libraries
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.model_selection import train_test_split
# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
# from sklearn.linear_model import LinearRegression
# from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
# from datetime import timedelta

# # 2. Load the dataset
# # Ensure the file 'delhi-weather-aqi-2025.csv' is uploaded to your Colab environment
# try:
#     df = pd.read_csv('delhi-weather-aqi-2025.csv')
#     print("Dataset loaded successfully!")
# except FileNotFoundError:
#     print("Error: Please upload 'delhi-weather-aqi-2025.csv' to the Colab files sidebar.")

# # 3. Data Preprocessing
# # Combine date and time into a single datetime object
# df['datetime'] = pd.to_datetime(df['date_ist'] + ' ' + df['time_ist'], dayfirst=True)
# df = df.sort_values('datetime')

# # Define Features and Target
# features = ['temp_c', 'humidity', 'co', 'no2']
# target = 'aqi_index'

# X = df[features]
# y = df[target]

# # Split data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # 4. Model Training and Comparison
# models = {
#     "Linear Regression": LinearRegression(),
#     "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
#     "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42)
# }

# print("\n--- Model Evaluation Results ---")
# best_model = None
# best_r2 = -float('inf')

# for name, model in models.items():
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)

#     mae = mean_absolute_error(y_test, predictions)
#     r2 = r2_score(y_test, predictions)

#     print(f"{name}:")
#     print(f"  - MAE: {mae:.2f}")
#     print(f"  - R2 Score: {r2:.4f}")

#     if r2 > best_r2:
#         best_r2 = r2
#         best_model = model
#         best_model_name = name

# print(f"\nBest Model: {best_model_name}")

# # 5. Predict Next 24 Hours
# # To simulate the "Next 24 Hours", we take the last known 24 entries or provide a way to forecast.
# # Here we demonstrate by predicting for the most recent 24-hour window in the data.

# def predict_24hr_forecast(model, last_data_sample):
#     # This assumes you have the weather/gas data for the next 24 hours.
#     # If not, you would typically use time-series forecasting (like LSTM/ARIMA).
#     predictions = model.predict(last_data_sample[features])

#     forecast_df = last_data_sample.copy()
#     forecast_df['predicted_aqi'] = predictions
#     return forecast_df[['datetime', 'temp_c', 'humidity', 'co', 'no2', 'predicted_aqi']]

# # Demonstration using the last 24 hours of the dataset
# last_24_hours_data = df.tail(24)
# forecast = predict_24hr_forecast(best_model, last_24_hours_data)

# print("\n--- Next 24-Hour Prediction (Hourly) ---")
# print(forecast)

# # 6. Visualization
# plt.figure(figsize=(12, 6))
# plt.plot(forecast['datetime'], forecast['predicted_aqi'], marker='o', color='red', label='Predicted AQI')
# plt.title(f'AQI Prediction for Next 24 Hours ({best_model_name})')
# plt.xlabel('Time')
# plt.ylabel('AQI Index')
# plt.xticks(rotation=45)
# plt.grid(True)
# plt.legend()
# plt.tight_layout()
# plt.show()

# # 7. Predict using your own test data
# def predict_custom_data(temp, hum, co, no2):
#     custom_input = pd.DataFrame([[temp, hum, co, no2]], columns=features)
#     prediction = best_model.predict(custom_input)
#     return prediction[0]

# print("\n--- Custom Prediction Tool ---")
# # Example usage:
# my_temp = 25.0
# my_humidity = 60
# my_co = 800
# my_no2 = 35
# result = predict_custom_data(my_temp, my_humidity, my_co, my_no2)
# print(f"Predicted AQI for Input (Temp:{my_temp}, Hum:{my_humidity}, CO:{my_co}, NO2:{my_no2}) is: {result:.2f}")

# # You can now call predict_custom_data() with any values to get an instant prediction.


import pandas as pd
import numpy as np
import pickle
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor

# 1. Load Data
df = pd.read_csv("delhi-weather-aqi-2025.csv")

# 2. Features: Now including the 2 API values and 4 Hardware values
# Order: [temp, humidity, pm25, pm10, no2, co]
features = ['temp_c', 'humidity', 'pm2_5', 'pm10', 'no2', 'co']
X = df[features]

#Target - 01 Regression - Find exact aqi value 
y_reg = df['aqi_index']


#Target - 02 Finding Aqi category (Classification)
def label_aqi(x):
    if x <= 50: return 0
    elif x <= 100: return 1
    elif x <= 150: return 2
    elif x <= 200: return 3
    elif x <= 300: return 4
    else: return 5

y_cls = df['aqi_index'].apply(label_aqi)

# 3. Train
X_train, X_test, y_train_reg, y_test_reg = train_test_split(X, y_reg, test_size=0.2, random_state=42)

_,_,y_train_cls,y_test_cls = train_test_split(X, y_cls, test_size=0.2, random_state=42)


scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)


# Train BOTH Models
print("Training Classification model...")
model_rf_cls = RandomForestClassifier(n_estimators=100, random_state=42)
model_rf_cls.fit(X_train_scaled, y_train_cls)

print("Training Regression model...")
model_rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
model_rf_reg.fit(X_train_scaled, y_train_reg)

# 4. Save to backend/app/models
current_dir = os.path.dirname(os.path.abspath(__file__))
model_dir = os.path.join(current_dir, '..', 'models')
os.makedirs(model_dir, exist_ok=True)

with open(os.path.join(model_dir, 'model_rf.pkl'), 'wb') as f:
    pickle.dump(model_rf_cls, f)

with open(os.path.join(model_dir, 'model_rf_reg.pkl'), 'wb') as f:
    pickle.dump(model_rf_reg, f)

with open(os.path.join(model_dir, 'scaler.pkl'), 'wb') as f:
    pickle.dump(scaler, f)

print(f"âœ… Model trained on 6 features and saved to {model_dir}")







